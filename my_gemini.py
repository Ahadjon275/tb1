#!/usr/bin/env python3
# https://ai.google.dev/


import base64
import random
import requests
import threading


import cfg
import my_log


# блокировка чатов что бы не испортить историю 
# {id:lock}
LOCKS = {}


# не принимать запросы больше чем, это ограничение для телеграм бота, в этом модуле оно не используется
MAX_REQUEST = 14000

# максимальный размер истории (32к ограничение Google?)
MAX_CHAT_SIZE = 25000


# хранилище диалогов {id:list(mem)}
CHATS = {}


def img2txt(data_: bytes, prompt: str = "Что на картинке, подробно?") -> str:
    """
    Generates a textual description of an image based on its contents.

    Args:
        data_: The image data as bytes.
        prompt: The prompt to provide for generating the description. Defaults to "Что на картинке, подробно?".

    Returns:
        A textual description of the image.

    Raises:
        None.
    """
    try:
        img_data = base64.b64encode(data_).decode("utf-8")
        data = {
            "contents": [
                {
                "parts": [
                    {"text": prompt},
                    {
                    "inline_data": {
                        "mime_type": "image/jpeg",
                        "data": img_data
                    }
                    }
                ]
                }
            ]
            }
        api_key = random.choice(cfg.gemini_keys)
        response = requests.post(
            f"https://generativelanguage.googleapis.com/v1beta/models/gemini-pro-vision:generateContent?key={api_key}",
            json=data,
            timeout=60
        ).json()

        return response['candidates'][0]['content']['parts'][0]['text']
    except Exception as error:
        my_log.log2(f'img2txt:{error}')
    return ''


def update_mem(query: str, resp: str, mem) -> list:
    """
    Updates the memory with a new query and response.

    Args:
        query (str): The query string.
        resp (str): The response string.
        mem (list): The memory list.

    Returns:
        list: The updated memory list.
    """
    if resp:
        mem.append({"role": "user", "parts": [{"text": query}]})
        mem.append({"role": "model", "parts": [{"text": resp}]})
        size = 0
        for x in mem:
            text = x['parts'][0]['text']
            size += len(text)
        while size > MAX_CHAT_SIZE:
            mem = mem[2:]
            size = 0
            for x in mem:
                text = x['parts'][0]['text']
                size += len(text)


def ai(q: str, mem = []) -> str:
    """
    Generate the response of an AI model based on a given question and memory.

    Parameters:
    - q (str): The question to be passed to the AI model.
    - mem: The memory of the AI model which contains previous interactions.

    Returns:
    - str: The response generated by the AI model based on the given question and memory.
    """
    mem_ = {"contents": mem + [{"role": "user", "parts": [{"text": q}]}],
            "safetySettings": [
            {
                "category": "HARM_CATEGORY_HARASSMENT",
                "threshold": "BLOCK_NONE"
            },
            {
                "category": "HARM_CATEGORY_HATE_SPEECH",
                "threshold": "BLOCK_NONE"
            },
            {
                "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                "threshold": "BLOCK_NONE"
            },
            {
                "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                "threshold": "BLOCK_NONE"
            }
        ]
            }

    keys = cfg.gemini_keys[:]
    random.shuffle(keys)
    for key in keys:
        url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=" + key
        response = requests.post(url, json=mem_, timeout=60)
        if response.status_code == 200:
            break

    try:
        resp = response.json()['candidates'][0]['content']['parts'][0]['text']
    except Exception as ai_error:
        resp = ''
        my_log.log2(str(response.json()))

    update_mem(q, resp, mem)

    return resp


def chat(query: str, chat_id: str) -> str:
    """
    This function is used to process a chat query and return a response.

    Parameters:
    - query (str): The chat query to process.
    - chat_id (str): The ID of the chat.

    Returns:
    - str: The response to the chat query.
    """
    if chat_id in LOCKS:
        lock = LOCKS[chat_id]
    else:
        lock = threading.Lock()
        LOCKS[chat_id] = lock
    with lock:
        if chat_id not in CHATS:
            CHATS[chat_id] = []
        mem = CHATS[chat_id]
        return ai(query, mem)


def reset(chat_id: str):
    """
    Resets the chat history for the given ID.

    Parameters:
        chat_id (str): The ID of the chat to reset.

    Returns:
        None
    """
    CHATS[chat_id] = []


def get_mem_as_string(chat_id: str) -> str:
    """
    Returns the chat history as a string for the given ID.

    Parameters:
        chat_id (str): The ID of the chat to get the history for.

    Returns:
        str: The chat history as a string.
    """
    if chat_id not in CHATS:
        CHATS[chat_id] = []
    mem = CHATS[chat_id]
    result = ''
    for x in mem:
        role = x['role']
        try:
            text = x['parts'][0]['text'].split(']: ', maxsplit=1)[1]
        except IndexError:
            text = x['parts'][0]['text']
        result += f'{role}: {text}\n'
        if role == 'model':
            result += '\n'
    return result    


def chat_cli():
    while 1:
        q = input('>')
        r = chat(q, 'test')
        print(r)


if __name__ == '__main__':
    chat_cli()
    
    # data = open('1.jpg', 'rb').read()
    # print(img2txt(data))